<!DOCTYPE html>
<html lang="ru">
 <head>
  <title> Рабочий дневник AI-1998d </title>
  <meta content="канал может быть информационно насыщенным и при этом он может абсолютно точно узнавать фазы Луны, но всё это не важно для бабочки" name="description">
  <meta content="время, громкость, изменения, канал, поведение, протокол, событие, узнавание, успех, " name="keywords">
  <meta content="Евгений Корниенко" name="author">
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <link href="https://cord70.github.io/cyber/study/AI-1998d.html" rel="canonical">
  <link href="https://cord70.github.io/cyber/favicon.svg" rel="icon" sizes="any" type="image/svg+xml">
  <link href="../images/main.css" rel="stylesheet">
  <script defer="" src="navStudy.js"> </script>
  <script defer="" src="../images/ansimeta.js"> </script>
 </head>
 <body>

  <header>
  </header>

  <main>
   <p class="headquote">
    Для того чтобы попугай легко узнавал свой
            голос желательно, чтобы каналы слуха и голоса были похожи по характеристикам
            восприятия. Но для того, чтобы голос попугая был узнаваем человеком,
            этот голос должен тоже иметь некоторые человеческие свойства. </p>

   <h2> Рабочий дневник 1998d </h2>

   <h3 id="BM1">
    01.04.98 </h3>

   <p> "Информационная насыщенность" и "решающий вес"
            не достаточны для оценки веса канала. Важность канала всё же в
            первую очередь определяется успехом. Например, канал может быть
            информационно насыщенным и при этом он может абсолютно точно узнавать
            фазы Луны, но всё это не важно для бабочки. И наоборот. Канал
            может плохо узнавать положение цветка и ошибаться в 90% случаев.
            Но если у бабочки нет другой информации о цветке, то такой "плохой"
            канал становится в 1000 раз важнее (нужнее), чем очень умный специалист
            по фазам Луны. </p>

   <p> Но успех анализируется в отложенном режиме. Поэтому надо запоминать
            какую-то информацию о том, как было принято решение об узнавании.
            Например, какие были веса, или какие каналы узнавали то успешное
            или неуспешное событие. Чтобы не расходовать память на "признак
            узнавания" можно восстанавливать "узнаваемость"
            прежнего (решающего) события функционально. Причём нужно проверять
            именно качество узнавания, а не качество прогноза. Так как от
            узнавания зависит формирование общего события (образа). А ретро
            прогноз у эффекторов часто бывает просто точным. Одновременно
            с оценкой узнавания текущего события проводим оценку узнавания
            события в точке оценки успеха. </p>

   <p> Один единственный канал "Успех" не может правильно
            взвесить все каналы. Пример. Пусть бабочка имеет 2 канала зрения,
            2 канала движения + канал Wish. Ясно, что оптимальная взаимозависимость
            каналов такова: </p>

   <p class="center">
    Желание -- &gt; Движение по X &lt;-- Зрение
            по X
    <br> Желание -- &gt; Движение по Y &lt;-- Зрение по Y </p>

   <p> Но
    <i> технология общих весов требует узнавания полного события </i> . Это не плохо, но весьма ресурсоёмко. Если же каждый канал
            узнаёт события по своему списку весов, то "экономное"
            узнавание возможно. </p>

   <ul> <li> Движение по X: вес зрения по X = 1; вес желания = 1 </li>
    <li> Движение по Y: вес зрения по Y = 1; вес желания = 1 </li>
    <li> Зрение по X: все веса = 0 </li>
    <li> Зрение по Y: все веса = 0 </li>
    <li> Желание: все веса = 0 </li> </ul>
   <p> На самом деле, когда канал X узнаёт общее событие, то для
            него это то событие, при котором зрение по X и желание были такими
            же, как сейчас. Зрение и движение по Y не имеет значения. Канал
            X выбирает из канала "успех" наиболее удачные события
            в эти узнаваемые моменты. Но, может быть, часть этих удач была
            вызвана правильной работой канала Y, а не X. Если даже так, то
            канал X тоже тогда вёл себя не плохо. Следовательно, нельзя считать,
            что канал Y вёл себя плохо. </p>

   <p> Следовательно, канал Успех "взвешивает" согласованную
            работу всех каналов и из неё нельзя выделить роль канала X или
            Y! Следовательно, нет смысла каждому каналу содержать свой список
            весов. Может быть, общий список не так эффективен, но иного не
            дано. Не может быть более чем один список на один канал успех.
    <b> Следовательно, нет смысла разделять биты X и Y в двухбитовом
                канале зрения. </b> </p>

   <p> Итак, веса определяет один канал Успех. Необходимость в разделении
            весов возникает, только если есть разные желания. Может ли сформироваться
            единая личность, если каналов "успех" слишком много?
            Можно подсчитывать "суммарное удовольствие" от удовлетворения
            многих желаний, и помещать это число в один канал Pleasure. </p>

   <ul> <li> Воспроизведение успешного поведения обеспечивается тем, что
                система правильно узнаёт
     <b> важные </b> события, и ведёт себя
                так, как ранее, когда она достигала успеха. Поиск нового поведения
                происходит на фоне освоенного. </li> </ul>
   <p> Веса нужны для обеспечения узнавания
    <b> важных событий </b> . Если только один канал из тысячи узнаёт важные события, а другие
            каналы узнают "маловажные события", то этот единственный
            канал и должен определять поведение. Его вес должен "перевешивать".
            А система должна уметь достаточно быстро выявлять такой канал
            из 1000 или даже из 1000000. </p>

   <p> Какие вообще-то возможны способы взвешивания и узнавания общего
            события. </p>

   <ul> <li> (+) Сумма весов </li> </ul>
   <ul> <li> Обязательное узнавание "важными каналами" + сумма
                по остальным каналам </li>
    <li> По сумме мест узнавших каналов (чем меньше, тем лучше). Это
                разновидность взвешивания. </li> </ul>
   <ul> <li> (-) Количество узнавших каналов (теряет смысл важность каналов) </li>
    <li> (-) Если события не пересекаются, то выбираем все, если пересекаются,
                то только пересечение (теряет смысл важность каналов) </li>
    <li> (-) Каждый канал вырабатывает не только "удачный ход",
                но и "удачный вес". Это приводит к угадыванию среди
                слишком большого количества вариантов. Обучение катастрофически
                замедляется. </li>
    <li> (-) Каналы расставлены по рангу. Первый узнающий. Так невозможно
                различать детали. </li> </ul>
   <p> Если событие не узнано, то имеет ли оно "важность"?
            Важность имеет канал, а не событие. То, что не узнано - не событие.
            Событие (или процесс) - это то, что входит в список Event. Но,
            строго говоря, все события в протоколе имеют ненулевой уровень
            узнавания. </p>

   <ul> <li> Пусть Система содержит 2 канала. Причём один из них в тысяче
                экземпляров. Пусть эта тысяча каналов узнала какое-то "мало
                важное" событие, а 1001
     <sup> й </sup> канал узнал важное
                событие. </li>
    <li> <i> Пусть веса были таковы, что первое событие было признано
                    общим. Это приведёт к ухудшению успеха. Поэтому веса этих 1000
                    каналов должны со временем уменьшиться. А вес канала, </i> <b> который не узнавал </b> <i> это "мало важное" событие </i> <b> не должен меняться </b> <i> . </i> </li>
    <li> Меняем веса только тех каналов, которые узнают событие. Плохая
                или хорошая оценка ставится только тем каналам, которые приняли
                участие в узнавании оцениваемого события. Поэтому вес правильного
                канала со временем превысит суммарный вес 1000 неудачных каналов.
                Если в этот момент правильный канал сможет проявить себя и угадать
                хорошее поведение, то его вес повысится. Если не сможет проявить
                себя, то его вес снизится. В случае полной дезориентации такой
                системы веса всех каналов будут постоянно колебаться около решающих!
                Пока какие-то каналы не проявят себя с лучшей стороны. </li> </ul>
   <p> Кроме того, если удачный канал поможет повысить успех, то
            такое успешное поведение сохранится в протоколе, и будет всё больше
            поводов для узнавания событий именно этим каналом. Так накапливаются
            шаблоны (экземпляры) правильного поведения. </p>

   <p> В случае успеха повышаем веса тех, каналов,
    <i> которые угадали
                событие </i> (это зависит от весов. Порочный круг!), приведшее
            к успеху.
    <i> При достижении контрольного срока канал "успех"
                вычисляет успех </i> и
    <i> вспоминает (или "определяет"),
                какие каналы его предугадали </i> . Их веса повышаются. Веса других
            каналов не изменяются. Вспоминаем не вес, а "кто принял это
            решение". Если успех
    <i> ниже среднего </i> , то есть это
            не успех, а неудача на фоне обычного состояния, то вес узнающих
            каналов снижаем, так как именно их узнавание привело к неправильным
            действиям. Есть вероятность, что они неправильно узнали это событие.
            У типичного события может быть свой характерный успех. Поэтому
    <i> средний успех </i> нужно рассчитывать по набору событий, узнающих
            один и тот же ход. </p>

   <p> Канал запоминает 1) событие, 2) факт узнавания (Это мнение
            системы. Уровни узнавания этого события в каждом канале с учётом
            веса канала.) Этот факт может быть реконструирован. Можно не тратить
            память на его запоминание. Для уточнения весов сравниваем успех
            S (это результат всей работы по узнаванию и прогнозу) со средним
            успехом Smid. Если S&lt;Smid, то каналы, узнавшие это событие
            снижают свой вес, если S &gt;Smid, то повышают. То есть анализируются
            и успехи и "неудачи", то есть слишком малые успехи.
            Вес не узнавших каналов не меняется. </p>

   <p> Узнанным считается то событие, которое имеет в системе максимальный
    <i> средний </i> вес среди узнающих его каналов. Если канал не
            узнал событие, то это не то же самое, что узнавание с весом 0.
            Если канал не узнал событие, то он таких событий не замечает,
            (зрение не различает запахов), и его мнение не надо учитывать. </p>

   <p> На самом деле, можно считать, что такой канал узнал событие
            с весом 0. Просто в системе должен быть порог узнавания W*, чтобы
            такие веса, как 0, 0.01, 0.0001 не рассматривать на фоне веса
            1. Если суммарный вес больше, чем W*, то событие считается узнанным.
            Аналогично, если событие в канале имеет вес больше некоторого
            порога, то канал сообщает о нём, как об узнанном событии. Непонятно,
            зачем этот порог? Если вес канала мал, то и его роль мала. </p>

   <p> На какую величину надо уточнять вес: </p>

   <ul> <li> W:=W * 1.01; Так вес W=0 становится особым. </li>
    <li> W:=W +1 или -1 </li>
    <li> W:=W+(S-Smid)/(Smax-Smin); S &ndash; величина успеха в момент t+dt,
                dt &ndash; контрольный срок </li>
    <li> W:=W * [1+0.5*(S-Smid)/(Smax-Smin)]; Здесь тоже нулевой вес
                приобретает излишний смысл. </li>
    <li> W:=W + R*(S-Smid); R &ndash; коэффициент узнавания </li>
    <li> W:=W + R*(S-Smid)/(Smax-Smin) </li> </ul>
   <p> "Вес" может стать и отрицательным, и очень большим.
            Это не ведёт к логическим трудностям, так как мы сравниваем суммы
            весов. Если ограничить веса только положительными значениями,
            то получится, что вес W=0 - это какой-то известный вес, имеющий
            особый смысл (смысл в том, что такой канал можно не учитывать).
            Отсюда следует, что минимальный успех S=0 тоже известен. Это нарушает
            правило перекалибровки. Творческая система не может знать, достигла
            ли она максимального успеха, и может ли быть "ещё хуже". </p>

   <p> Успех S равен сумме удовольствий P (эта величина может быть
            отрицательной) за контрольное время dt. Использовать в качестве
            успеха "нарастание удовольствия" нельзя. Получится,
            что при нормальном самочувствии системы она может считать, что
            её поведение неуспешное. А есть ли смысл считать успехом интеграл
            от удовольствия - суммарное удовольствие за всю предыдущую жизнь?
            Нет, так как эта величина автоматически нарастает, и теряется
            инвариантность времени. Но интеграл за один и тот же интервал
            времени не нарастает. </p>

   <p> Строго говоря, если канал узнал событие, то в опыте этого
            канала таких событий было много. Мы не ищем их все (но алгоритм
            не должен измениться, если бы мы искали их все), и не знаем "процент
            не узнанных событий". Поэтому коэффициент узнавания - это
            надуманная вещь. Достаточно, если R=1 при узнавании, и R=0 при
            не узнавании. То есть "порог узнавания" следует из универсальности. </p>

   <p> Может быть R=1, если узнавание продолжается, R=1/2, если это
            новое событие, R=1/4, если узнавание только что прервалось, R=0,
            если узнавания не было и нет. Но к какому событию из всего образа
            это относится. Можно выбрать событие случайно, как при прогнозировании.
            Кроме того, если один канал узнаёт 1-2 события, а другой 1000000-2000000
            событий, то их вклад в вес может катастрофически различаться.
            Это приведёт к тому что, то один, то другой канал будут полностью
            выпадать из процесса. Это несправедливая конкуренция. </p>

   <p> Сложность такого алгоритма состоит в том, что некоторые каналы
            могут необратимо исчезнуть. Пусть порог узнавания равен 0.7. Четыре
            канала имеют веса 0.4, 0.4, 0.1 и 0.1. Узнавание возможно только
            если оба первых канала узнают текущее событие 0.4+0.4=0.8 &gt;0.7.
            Третий и четвёртый канал полностью не рассматриваются, а при уточнении
            весов их вес может только уменьшиться, так как при узнавании он
            не меняется. </p>

   <p> Если канал "узнал" какое-то событие вне общего списка,
            то это не говорит о неправильной работе канала или о "неправильном
            узнавании". Это говорит о том что,
    <i> по мнению канала </i> , текущее событие похоже на многие прежние события. Но вместе
            с тем это может отвлекать внимание от более важного общего события.
            Итак, характеристикой неузнавания общего события является наличие
            событий вне общего списка. Их количество не имеет значения. </p>

   <p> Допустим, что общими (наиболее похожими на текущее событие)
            признаны какие-то пять событий. Канал узнаёт 25 событий. Из них
            4 совпали с общими событиями. Значит, этот канал не заметил 1
            важное событие, 4 раза узнал общее событие правильно, 21 раз попытался
            отвлечь внимание от важного события. Но для него все 25 событий
            равны. </p>

   <p> Примеры. Пусть канал всегда всё узнаёт. Тогда его вес будет
            увеличиваться вместе с хорошими каналами. И будет уменьшаться
            вместе со всеми каналами при неудачах. Он постепенно отстанет
            от хороших каналов, так как они узнают важные события чаще, чем
            остальные каналы. Пусть канал никогда ничего не узнаёт. Это значит,
            что его события не попадают в общий список, в общий образ. Это
            почти невозможно, особенно для битовых каналов. Тогда его вес
            не имеет значения! Даже если он кажется слишком большим. Пусть
            канал надёжно узнаёт очень редкое и маловажное событие. Тогда
            его вес со временем снизится. </p>

   <p> Признак узнавания анализируется в отложенном режиме вместе
            с успехом. Он не может накапливаться или сглаживаться, чтобы анализироваться
            в текущем времени. Например, два канала могут иметь одинаковую
            среднюю степень узнавания, но узнавать разные (по важности) события.
            В том числе, у типичных событий может быть типичный уровень успеха.
            Тогда в текущем времени их веса должны меняться синхронно, но
            это неверно. </p>

   <p> Не может ли получиться, что вес одного хорошего канала станет
            слишком большим? Если он узнаёт всегда всё точно, и получаются
            одни (всё лучшие и лучшие!) успехи, то может. Но если бывают и
            неудачи, то вес снизится. Если хороший канал некоторое время не
            будет предсказывать успех, то его вес снизится до "решающего"
            веса! То есть, канал не будет потерян. Он начнёт конкурировать
            с плохими каналами, и их веса будут решающими поочерёдно, пока
            не будет получен успех. А это произойдёт с большей вероятностью
            благодаря хорошему каналу, так как он помнит правильное поведение,
            и узнаёт полезные ситуации. И его вес вновь увеличится. Не может
            быть, чтобы вес какого-то канала стал необратимо ниже (выше) всех.
            Если не учитывать влияние малого веса канала, то это может быть.
            Неузнавание - это не только слабое узнавание, но и пренебрежение
            данным каналом из-за его малого веса. Даже если он и узнавал событие,
            никто этим не воспользовался, и его вклад в успех не известен. </p>

   <p> Канал успех выделяет "общее" событие. А каждый канал
            может определять признак узнавания общего события. Это нечто вроде </p>

   <p class="big center">
    R = Sum(Echannel * Ecommon) </p>

   <p> Корректировка веса канала происходит по формуле </p>

   <p class="center">
    <span class="big">
     W:=W + R*dW </span> ; R
            &ndash; коэффициент узнавания </p>

   <p class="center">
    <span class="big">
     dW= (S-Smid)/(Smax-Smin) </span> ; dW &ndash; вклад успеха </p>

   <p> Если веса общих событий учитываются при прогнозе хода, то
            почему вероятность прогноза пропорциональна весу? Потому что веса
            разных каналов, в конце концов, складываются в процедуре выбора
            общих событий. Но если вес канала может быть отрицательным, то
            и вес каких-то общих событий может быть отрицательным. А общие
            события используются при прогнозировании с пропорциональными весами.
            Поэтому в вес общего события не должны входить веса каналов! Это
            разные пространства.
    <i> Веса в образе должны быть больше нуля
                и должны быть пропорциональны вероятности событий. </i> </p>

   <h3 id="BM2">
    Симметрия успеха </h3>

   <p> Пусть контрольное время составляет 20 шагов. Так как бабочка
            то подлетает к цветку, то улетает от него, то успехи будут распределены
            в интервале от 0 (ни одного успеха) до 20 (всё время на цветке).
            Все варианты от 1 до 19 будут равномерно распределены между поведениями
            "удаления от" цветка и "приближения к" цветку.
            Как надёжно отобрать поведение "приближения к" цветку?
            Всё решает
    <i> желание </i> бабочки! </p>

   <ul> <li> При прочих равных условиях
     <b> бабочка выбирает "приближение",
                    если желание не удовлетворено </b> . То есть
     <i> среди одинаково
                    успешных вариантов </i> бабочка ассоциативно предпочитает приближение
                к цветку, если её желание не удовлетворено и, скорее всего, "предпочитает
                случайное поведение" (так как все поведения распределены
                равновероятно), если её желание удовлетворено. </li>
    <li> Более строго, для выбора поведения учитывается канал "касания
                цветка", а не канал желания. А величина желания Wish рассчитывается
                по значениям в канале касания цветка. Поэтому сам дублированный
                канал Wish можно не использовать при ассоциативном поиске. Он
                нужен только для расчёта успеха. </li> </ul>
   <p> Следовательно, в первую очередь надо отбирать максимально
            узнаваемые события с порогом 1. А уже среди них отбирать более
            успешные с S &gt;Smid. </p>

   <p> Взвешенное прогнозирование, или прогнозирование с учётом величины
            успеха, важно не потому, что данный ход, за которым следует лучшее
            будущее, сам по себе правильный, а потому, что этот ход сохраняет
            ассоциацию с лучшей цепочкой действий, приводящей к лучшему будущему.
            Может быть, именно этот ход не самый правильный, но он позволяет
            не забыть (вспомнить) цепочку действий, которая содержит большее
            количество правильных ходов. Вот эта цепочка и есть поведение,
            которое постепенно оптимизируется путём случайного творческого
            поиска. </p>

   <p> При сжатии протокола могут быть утеряны причинные цепочки,
            приводящие, в конце концов, к успеху. Пример. Пусть поведение
            приводит к синусоидальному характеру удовольствия. При этом успех
            тоже будет синусоидальным. Поскольку успех есть интеграл от удовольствия,
            то он будет отставать по фазе. Пусть он отстаёт по фазе на 180&deg;,
            так что максимальному успеху соответствует минимальное удовольствие.
            Если при сжатии протокола удалить все участки с минимальным успехом,
            то
    <b> будут удалены и участки с максимальным удовольствием </b> , и с поведением непосредственно перед удовольствием, в момент,
            и после этого удовольствия. При выработке поведения на основе
            такого ущербного протокола в тот момент, когда нужно сделать действия,
            приводящие к максимальному удовольствию, существо будет делать
            действия, которые оно выполняло ранее после окончания получения
            этого удовольствия, что, скорее всего, совершенно неуместно. Вместо
            обеда оно скажет спасибо, и встанет из-за стола. :) </p>

   <p> Отсюда следует, что локально, в момент максимального удовольствия
            поведение не может быть абсолютно сознательным. В общем, чем продолжительнее
            контрольное время, и чем разнообразнее жизнь существа, тем менее
            вероятны такие казусы. </p>

   <h3 id="BM3">
    Слух и голос </h3>

   <p> Перед началом работы проводим автокалибровку громкости. Подбираем
            такую громкость, чтобы сигнал в микрофоне был средним. Результат
            обучения обязательно запоминаем в файл. Так как оно будет долгим.
            Надо программным путём отключить все звуковые входы кроме микрофона,
            а при выходе из программы - всё восстановить, в том числе и громкость. </p>

   <p> Как выбрать инструмент MIDI? Можно использовать несколько
            каналов одновременно. Флейта имитирует гласные, а ударные - согласные.
            (По умолчанию все звуковые платы исполняют тембр фортепьяно.) </p>

   <p> Для ускорения набора (и уменьшения) громкости нужна логарифмическая
            шкала. Это стандарт МИДИ. Канал выдаёт 1 бит, имеющий смысл второй
            производной "намерения". Если намерение сохраняется,
            то скорость нарастания громкости увеличивается. Такая же логика
            годится для всех аналоговых каналов, работающих под управлением
            битовой команды. Для инструментов типа фортепьяно трудно управлять
            временем затухания звука. Это механическая характеристика, исполняемая
            звуковой платой. </p>

   <p> Прежде чем экспериментировать со звуком, надо отладить аналоговый
            датчик (поворота) на бабочке (стрекозе). </p>

   <p> Нужно ли выдавать звуки по таймеру, или по тактам? У меня
            в программе такты определяются таймером, и могут нарушиться, если
            расчёт не успевает. Чтобы не зависеть от скорости вычислений нужно
            выдавать звуковую команду в начале такта. А после этого слушать
            и делать другие расчёты. </p>

   <ol>
    <li> По началу такта, когда приходит событие от таймера, в "звуковых"
                каналах создаём MIDI звук из выработанных попугаем команд. Например,
                нота, громкость, инструмент. </li>
    <li> Запускаем этот звук на исполнение. Поэтому голос попугая
                звучит всё время. Но громкость может быть равна 0. При "внезапном"
                возрастании громкости можно имитировать согласные МИДИ средствами. </li>
    <li> Включаем запись с микрофона по каналу WAV на 0.01 секунды. </li>
    <li> Анализируем Wav звук, и получаем простейший спектр. Например,
                3 полосы с 8 уровнями громкости в каждой полосе. Или с "аналоговым"
                каналом в полосе. </li>
    <li> Передаём эти данные в канал "слух" попугая. Нет
                смысла растягивать этот процесс на последовательные такты. </li>
   </ol>
   <p> Фильтр низких частот: Uнч = 1/(2T)
    <b> Integral </b> {Udt}
            от t-T до t+T </p>

   <p> Фильтр высоких частот: Uвч = U-Uнч </p>

   <p> Частота среза этих фильтров примерно определяется из формулы
            wT =1 (w - это греческая омега). </p>

   <p> Если U=Uo Sin(wt), то Uнч(w)=Uo Sin(wT)/(wT) </p>

   <p> Если применить фильтр низких частот для интервала T, а затем
            для интервала 2T, то получится октавный фильтр с характеристикой </p>

   <p class="big center">
    Uoct(w)=Uo [1-Cos(wT)] Sin(wT)/(wT) </p>

   <p> В арсенале МИДИ есть широкополосные звуки, похожие на белый
            шум. Есть и более простые (по расчётным ресурсам) октавные фильтры,
            чем эта формула. </p>

   <p> Для того чтобы попугай легко узнавал свой голос желательно,
            чтобы каналы слуха и голоса были похожи по характеристикам восприятия.
            Но для того, чтобы голос попугая был узнаваем человеком, этот
            голос должен тоже иметь некоторые человеческие свойства. Например,
            слух и голос могут содержать по 5 октавных частотных полос со
            средними частотами 200, 400, 800, 1600, 3200 Гц. Но голос не должен
            буквально озвучивать эти 5 тонов. Это просто, но недоступно человеческому
            слуху. Нужно выдавать 4 "точных" тона + 5 шумовых "полутонов".
            Точные тона могут определяться по уровням a и b в соседних полосах,
            как по весам. </p>

   <p class="big center">
    ln(f) = [a ln(fa) + b ln(fb)]/(a+b); s
    <sup> 2 </sup> =a
    <sup> 2 </sup> +b
    <sup> 2 </sup> </p>

   <p> Если дан минимальный интервал дискретизации времени Dt, то
            минимальная и максимальная распознаваемая частота есть </p>

   <p class="big center">
    Fmin=1/T; fmax=1/(4Dt); T=N Dt </p>

   <p> Пусть fmin=100Hz, fmax=10000Hz. Тогда </p>

   <p> T=0.01sec, Dt=0.000025sec, N=400. </p>

   <p> Центральная частота fo=sqrt(fmin * fmax)=1000Hz </p>

   <p> Фильтр низких частот на 1000 Гц получается при суммировании
            по 20 точкам. Такое суммирование можно сделать текущим по выборке
            из 400 точек. Итого число операций получается порядка 400. При
            воспроизведении через устройство Midi выбирается близкая нота
            и тембр флейты для чистого тона. А какой тембр для шума? </p>

   <p> <b> Более эффективный алгоритм октавного фильтра </b> . Принимаем
            осциллограмму из 512 чисел. Рассчитываем дисперсию Do. Это квадрат
            суммарного уровня во всём диапазоне частот. Усредняем входные
            данные по парам точек и получаем 256 точек. Дисперсия для этих
            точек - это уже квадрат суммарного уровня для частот от самой
            низкой до половины максимальной. И так далее. После получения
            всех таких уровней можно вычислить точные уровни в октавных полосах,
            но для разборчивости слуха это не требуется. Этот алгоритм легко
            сделать на ассемблере. </p>

   <p> Какие именно характеристики человеческого голоса обеспечивают
            разборчивость и узнавание речи? Частота, темп, ритм, изменение
            тона, изменение громкости? </p>

   <p> Можно запоминать "фонемы" по 0.1 секунды с номерами.
            Бабочка использует не ноты MIDI, а готовые фонемы, как буквы в
            монологовом попугае. При произнесении фонемы должны сливаться
            по определённой технологии расчёта вывода по WAV каналу. </p>

   <h3 id="BM4">
    Аналоговый орган </h3>

   <p> Каждое изменение входного сигнала нужно кодировать в биты
            (например, одного байта) так, чтобы каждый бит имел смысл. Чем
            больше бит требуется для получения осмысленной величины, тем дольше
            обучение. Например </p>

   <ol>
    <li> X1 - Первый бит. Сигнал растёт (1) или не растёт (0) </li>
    <li> 1X - уменьшается </li>
    <li> 00 - не меняется (недогрузка по чувствительности) </li>
    <li> 11 - перегрузка (неопределённость) </li>
    <li> X1XX - производная растёт </li>
    <li> 1XXX - производная убывает </li>
    <li> 00XX - производная не меняется </li>
    <li> 11XX - перегрузка по производной </li>
   </ol>
   <p> Всего достаточно 4 бита. А без производных - 2. </p>

   <p> Такой же аналоговый канал должен обслуживать эффектор. При
            этом могут возникнуть внутренние противоречия. Например, сигнал
            растёт и одновременно уменьшается. Надо бы разработать непротиворечивый
            протокол. Часть битов эффектора могут быть не выходами, а входами.
            Например, бит перегрузки, или индикатор недостаточной чувствительности.
            Тут пригодилась бы трёхзначная логика: налево, прямо, направо;
            растёт, не меняется, убывает; недогрузка, норма, перегрузка. </p>

   <p> В идеале аналоговый канал должен быть перенормируем, то есть
            не должен иметь характерного масштаба. Но практически это невозможно,
            так как реальный датчик имеет порог чувствительности (первый масштаб)
            и порог перегрузки (второй масштаб). В нормальных условиях сигнал
            этих пределов не достигает, и получается впечатление перенормируемости. </p>

   <p> Внутри рабочего диапазона характерные масштабы возникают всё
            равно, так как происходит сравнение хода разных физических процессов.
            Это "относительные" масштабы. Температура по сравнению
            с температурой кипения воды. Размер по сравнению с размером своего
            тела. </p>

   <p> Смысл должен содержаться в каждом бите. Если для получения
            смысла требуются последовательные биты, то это эквивалентно широкому
            потоку бит, и обучение замедлится. Хотя для придания смысла каждому
            биту орган (датчик) может использовать историю. Мозг об этом ничего
            не знает. Например, если бит означает "сигнал растёт",
            то в датчике это было установлено с использованием истории. </p>

   <p> Есть смысл динамически менять чувствительность аналогового
            датчика. Слишком малые изменения не регистрируются, но при этом
            постепенно уменьшается сам критерий "малости изменения".
            Насекомые и пресмыкающиеся не видят как неподвижные, так и слишком
            медленно перемещающиеся предметы. </p>

   <p> Для того чтобы медленные изменения всё же заметить, надо сравнивать
            уровень не с предыдущим уровнем, который может медленно (незаметно)
            уплывать, а с тем, который был зафиксирован в момент последнего
            изменения уровня. </p>

   <p> <b> В природе нет количеств - только качества. Следовательно,
                калибровочная инвариантность - недостаточно общий принцип. </b> </p>

   <p> Достаточно таких событий: сигнал зарегистрирован, датчик адаптировался,
            изменение сигнала зарегистрировано. Даже увеличение и уменьшение
            сигнала - нарушают калибровочную общность. Но если регистрируются
            только моменты изменения, то должна узнаваться очень длинная история.
            Например, удовольствие появляется раз в 100 ходов. А если темп
            изменится на 1 шаг? </p>

   <h3 id="BM5">
    Типы органов </h3>

   <p> <b> Основной, битовый канал </b> . Значения 0 и 1, да или
            нет: лево - право; касание - нет касания. </p>

   <p> <b> Аналоговый орган </b> (тепло, громкость, яркость) - это
            набор битовых каналов. Чувствительность, динамический диапазон,
            и шаг предсказания динамически подстраиваются под текущий сигнал. </p>

   <p> <b> Спектральный орган </b> (слух, голос, спектральные полосы
            фиксированы) - это набор (массив) аналоговых органов. Для каждого
            спектрального канала применяется аналоговый орган. Но возможен
            не только ассоциативный, но и мрямой обмен между каналами. Например,
            канал может использовать данные из другого канала, так как они
            имеют точно такую же физическую природу. </p>

   <p> <b> Сканирующий </b> аналоговый или спектральный орган (зрение,
            перемещение в пространстве). Сканирование может быть как геометрическим
            (перемещение, поворот луча зрения, вращение поля зрения, изменение
            масштаба, изменение фокусировки), так и в фазовом пространстве
            (изменение центральной частоты спектральной полосы). </p>

   <p> <b> Символьный орган </b> . Состоит из набора битовых органов.
            Но каждый набор битов имеет независимый смысл. Например, у бабочки
            на вебе зрение и движение - это двухбитовые символьные органы.
            Обучение такого органа очень не эффективно, так как каждый бит
            в отдельности не имеет смысла. Зато такой орган позволяет создавать
            семантическое управление за счёт присвоения нового символьного
            значения повторяющейся последовательности уже освоенных символов.
            В перспективе, для повышения интеллектуальности потомков от битового
            органа нужно, чтобы битовый орган имел возможность самосовершенствоваться
            до символьного органа. При этом выход битового канала остаётся
            двузначным, и семантический смысл скрыт от исполнительного механизма. </p>

   <p> Все механизмы (машины) устроены семантически. То есть мы придаём
            определённый сжатый смысл каждому процессу и каждой детали машины.
            Этот пример показывает, что семантика не обязательно рождает сознание.
            Например, компьютер оперирует символами логичнее, чем человек,
            но не имеет сознания. Наоборот, сознание создаёт семантику. Но
            первичное сознание, которое ещё не изобрело язык, может оперировать
            только качественными категориями, такими как, ощущения, впечатления,
            желания. Детализация этих качеств уже подготавливает создание
            семантики. Эта детализация представима в виде цветов, звуков,
            запахов. А главное - короткие ассоциации. </p>

   <p> В зависимости от физической природы органа каналы могут пользоваться
            опытом "ближайших соседей" или вообще любых других каналов
            из этого же или другого физически подобного органа. Использование
            чужого опыта ускоряет обучение благодаря тому, что каждый "бит"
            канала "содержит" гораздо больше бита информации. Ширины
            "спектральных" полос должны быть достаточно большими,
            чтобы возникла избыточность и перестраховка от потери и искажения
            информации. Например, на узкополосный синусоидальный сигнал в
            органе слуха откликаются сразу сотни рецепторов. Этим также обеспечивается
            обучение "менее чем по одному биту". </p>

   <p> В спектральном органе канал может использовать опыт другого
            канала, так как они имеют абсолютно одинаковую физическую природу.
            Это не сравнение зрения со слухом. Это сравнение первой октавы
            со второй. Однако при поиске ассоциаций в соседних 10 каналах
            время расчёта возрастает в 10 раз. Для ускорения можно продолжать
            сравнение в одном канале пока сохраняется узнавание, а при потере
            узнавания проверять за такт только один из 10 доступных каналов.
            Этот механизм статистически не упускает события в матрице датчиков. </p>

   <p> Слух - это линейка датчиков. Зрение - это матрица. </p>

   <h3 id="BM6">
    Зрение </h3>

   <p> Орган зрения должен быть физически или алгоритмически устроен
            так, чтобы физические параметры изображения узнавались при изменении
            масштабов. Это свойство называется калибровочной инвариантностью.
            В частности, зрение должно быть инвариантно по яркости, сдвигу
            (параллельному перемещению наблюдаемого объекта), вращению, масштабу
            (размеру), зеркальному отражению, фокусировке. Как обеспечиваются
            эти свойства в природе, и как изготовить правильный искусственный
            орган зрения? Забегая вперёд, скажу, что все эти свойства довольно
            легко реализуются алгоритмически в описанном выше сканирующем
            органе. </p>

   <p> Как глаз (нейронная сеть, сетчатка) приучается к инвариантности
            указанных параметров. </p>

   <p> <b> Яркость </b> . Изменение яркости точки, воспринимаемой
            отдельным светочувствительным нейроном происходит при переводе
            взгляда с одной точки объекта на другую, или при естественном
            изменении яркости этой точки. Поскольку это явление встречается
            постоянно, то есть повторяется миллионы раз, то глаз обучается
            тому, что некоторые изменения яркости не означают изменения самого
            объекта. Происходит узнавание объекта, несмотря на некоторые вариации
            его освещённости. </p>

   <p> <b> Сдвиг </b> . Глаз приучается к сдвиговой инвариантности
            благодаря инстинктивному сдвиговому сканированию. При этом впечатление
            о сдвиге объекта возникает в результате обобщения впечатления
            об определённом изменении освещённости видимого изображения. При
            сканировании (саккадировании) происходят точно такие же изменения
            освещённости сетчатки, как и при физическом параллельном перемещении
            объекта. </p>

   <p> <b> Вращение </b> . Возможно, глаз совершает небольшие вращательные
            движения при сканировании. Во всяком случае, это бы не помешало
            для накопления опыта. Некоторое вращение поля зрения происходит
            при движениях головы. Но всё это относится к полю зрения, а не
            к объектам. Способность узнавать объекты, несмотря на их поворот,
            требует привлечения функций сознания, а не только зрения. </p>

   <p> <b> Масштаб </b> . Приучение к узнаванию объекта, который
            изменяет свой размер, происходит в результате накопления опыта
            об изменении распределения яркостей на объекте при его небольшом
            удалении и приближении. </p>

   <p> <b> Зеркальное отражение </b> . Это невозможно механизировать.
            Узнавание отражённого объекта происходит благодаря сознанию: мысленный
            поворот, мысленное отражение. </p>

   <p> <b> Фокусировка </b> . Так же как и при масштабировании. </p>

   <p> Почти для всех этих свойств требуется накопление опыта. Закроем
            половину сетчатки и обучим только открытую половину. После открывания
            всей сетчатки ранее закрытая половина должна пройти полное обучение.
            Опыт открытой сетчатки вдоль первого слоя обычной нейронной сети
            не передаётся. В общем случае это не страшно, так как в процессе
            этого повторного обучения глаз будет видеть почти нормально. Но
            можно сделать датчик зрения лучше, чем нейронная сеть. </p>

   <p> Построим сетчатку на основе сканирующего аналогового органа.
            В таком органе каналы могут пользоваться опытом соседних каналов.
            Поэтому, во-первых, опыт частично распространяется вдоль сетчатки.
            Во-вторых, такая передача опыта позволяет обобщить любые небольшие
            искажения изображения одинаковым методом. Например, масштаб, вращение,
            фокусировка и сдвиг становятся разновидностями малых искажений.
            Большие изменения калибровочных масштабов есть просто сумма малых. </p>

   <p> Слух + голос дополняют друг друга. Система может слышать себя.
            Можно придумать органы зрения и "показа", позволяющие
            видеть себя. При этом тоже возникнут осознаваемые обратные связи.
            В чём может состоять желание такой системы? Сможет ли такая система
            повторить показанное ей изображение? </p>

   <h3 id="BM7">
    Слух </h3>

   <p> Слух должен быть инвариантным по громкости спектральной полосы
            и по сдвигу частоты (перетеканию громкости между полосами). Навык
            узнавания изменения громкости может быть одинаковым для всех каналов.
            Это значит, что они могут использовать опыт друг друга. Опыт перетекания
            громкости может использоваться только для соседних (по частоте)
            каналов. Это позволит узнавать звуки с изменяющимся и изменившимся
            тоном. </p>

   <p> Практически можно принимать каждый фрейм от звуковой платы
            с темпом до 40 Кгц. И по мере необходимости (10 раз в секунду)
            вычислять спектры. Но это слишком ресурсоёмко. Можно сделать специальные
            hardware аналоговые фильтры, и считывать амплитуды сигналов с
            темпом 10 Гц. Это самое эффективное решение. Но оно сильно затрудняет
            демонстрацию. Можно принимать готовые спектры от специальной звуковой
            платы. Это лучше, но такие платы недостаточно широко распространены.
            Наконец, наиболее приемлемый на сегодня (инженерный) метод. Принимаем
            фреймы с темпом 5-8Кгц 10 раз в секунду по 0.01 секунды. То есть,
            пропускаем 90% звука. В этом случае их можно успеть обработать
            ещё за 0.01 секунды. Применяя октавный цифровой фильтр можно получить
            4-6 спектральных полос. Столько и потребуется аналоговых датчиков. </p>

   <p> Выдаём звук без разрывов. Не беда, что попугай слышит только
            10% времени, он об этом не может догадаться. </p>

   <h3 id="BM8">
    Темп </h3>

   <p> Как возможно узнавание событий, которые происходят в той же
            последовательности, но в другом темпе? Такие события могут локально
            (по времени) узнаваться, но узнавание обычно будет теряться. Требуется
            много образцов с разными темпами, чтобы надёжно их узнавать. Но
            при этом они не будут считаться одним и тем же событием, а будет
            "много разных событий". Так же для бабочки движение
            вверх, и движение вниз &ndash; это совершенно разные вещи. И нет такого
            понятия, как "движение к цветку". </p>

   <p> Если темпы двух событий отличается не сильно, то окрестность
            каждой точки события становится узнаваемой, так как фактически
            узнавание "квантуется" благодаря наличию порога узнавания. </p>

   <h3 id="BM9">
    Мёртвое время медленных датчиков. </h3>

   <p> Пусть канал передал команду эффектору. Эффектор прочитал команду
            и приступил к исполнению. Физически он устроен так, что некоторое
            время после этого он не реагирует на новые команды. Как это скажется
            на работе всей системы? </p>

   <ol>
    <li> Система обнаружит, что её действия после команды не имеют
                значения для достижения успеха. И может снизить важность такого
                канала. </li>
    <li> Если этот канал на самом деле важный, то команда тоже важна,
                и некоторое время после неё и до неё тоже важно (для узнавания).
                Поэтому работа канала вхолостую (фантазии) после выдачи правильных
                команд будут запоминаться и повторяться. Система не может отличить
                их от реальности на основе своих ощущений, то есть на основе анализа
                этого одного канала. Но если система может наблюдать действие
                этого эффектора через другие каналы, то она сможет логически отличить
                свои реальные действия от фантазий, не сопровождаемых действиями,
                и назовёт такие фантазии "мыслями" или сном. </li>
   </ol>
   <p> Канал не может быть не важным для прогноза и при этом важным
            для действия. Процесс засыпания (отвлечения от реальности) и пробуждения
            (переход от мыслей к реальным действиям) важен и должен узнаваться
            в этом канале. </p>

   <h3 id="BM10">
    Инвариантность темпа. Решение. </h3>

   <p> Канал не обязан хранить все данные в детальном формате, так
            как эта детальность всё равно не используется. Данные используются
            только для сравнения друг с другом внутри одного канала. Поэтому
            допустимо квантование входной информации. </p>

   <p> Например, на вход поступает угол в градусах в форме числа
            Real. В канале применяется узнавание угла с точностью до квадранта,
            то есть углы 10&deg; и 80&deg; считаются равными. Можно включить процедуру
            приведения исходных углов к одному квадранту в процедуру узнавания.
            А можно сразу запоминать квантованные данные. То есть запоминать
            только одно число из списка: 0&deg;, 90&deg;, 180&deg;, 270&deg;. Или просто запоминать
            одно из 4 чисел 1,2,3,4. "Близкими"
    <i> считаются такие
                углы, которые совпадают. </i> <b> Вместо порога узнавания вводится
                квантование измерения. </b> </p>

   <p> При такой технологии в канале будут часто встречаться повторяющиеся
            коды. Реальный угол плавно возрастает от &ndash;10&deg; до +100&deg;, а в канале
            сначала будет записана сотня углов 0&deg;, а потом полсотни углов
            90&deg;. То есть канал всё же хранит данные, а не "ассоциации".
            Ассоциации возникают при узнавании данных. </p>

   <p> Смысл разных темпов состоит в том, что такие ступеньки для
            процессов, отличающихся только темпом, будут совершенно одинаковыми
            по последовательности исполнения, но отличающимися по длительности. </p>

   <p> Сейчас у меня в программе узнавание происходит по тактам.
            Если одна и та же точка протокола некоторое время узнаётся, то
            текущий процесс и по величинам и по темпу совпадает с узнаваемым
            процессом. </p>

   <p> Если считать точку узнанной на основании того, что она узнавалась
            на предыдущем шаге, и текущее значение совпадает со значением
            на следующей ступеньке, то одинаковые процессы с разными темпами
            будут считаться одинаково узнаваемыми. В момент такого темпо-инвариантного
            узнавания точка узнавания перескакивает на следующую ступеньку. </p>

   <p> Для того чтобы не было проблем с перескакиванием и со сканированием
            вдоль ступенек надо запоминать ступеньку как одно "состояние",
            характеризуемое </p>

   <ol>
    <li> значением </li>
    <li> интервалом времени. Данные отсортированы по времени. При
                узнавании находим t и сравниваем данные. Как это хранить короче? </li>
    <li> признаком узнавания (наверно, не обязательно). </li>
   </ol>
   <p> Входные данные считаются впервые узнанными, если они совпадают
            со значением на ступеньке. Узнавание считается продолжающимся,
            если входные данные продолжают совпадать со ступенькой, или совпадают
            со следующей ступенькой. В последнем случае точка узнавания перемещается
            в следующую ячейку. </p>

   <p> В таком процессе события с разными темпами могут стать неразличимыми.
            Но если событий (и каналов) много, то создаётся фон с "естественным
            темпом" событий. На этом фоне система может узнать событие,
            протекающее с необычным для него темпом, понимая, что этот темп
            необычен. Само изменение темпа будет замечено. А именно, канал
            сенсора может вводить данные не по тактам, а по команде от датчика
            об изменении сигнала. </p>

   <p> Теперь каналы могут работать асинхронно! Но общее физическое
            время всё равно используется для синхронизации узнавания. </p>

   <p> Если все события, совпадающие по значению с текущим событием,
            узнаются одинаково, то вероятность прогнозирования изменения события
            (перехода от одной полки к другой) всегда остаётся слишком малой,
            так как такой переход следует только за одной точкой из всей полки
            - последней. </p>

   <p> Надо ли узнавать "место на полке"? Нет. Вероятность
            окончания полки отражает реальную длительность полки. </p>

   <p> Хорошо бы, чтобы событие с правильным (это противоречит инвариантности)
            темпом и с правильной предысторией узнавалось лучше. Это получится,
            если вес узнаваемых событий увеличивать от такта к такту. "Правильность"
            события определяется не по его индивидуальному темпу, а по ассоциации
            с другими событиями. </p>

   <p> Физическое, сохраняемое вместе с данными время уже не обязано
            начинаться в минус бесконечности. Допустимо "периодическое
            время", например от 0 до $FFFF, так как вероятность точного
            попадания близких по смыслу событий на одни и те же места в разных
            периодах весьма мала. </p>

   <p> В живом мозгу тоже есть периодические процессы "волны".
            The first is the "neurobiological theory of consciousness"
            outlined by Crick and Koch (1990; see also Crick 1994). This theory
            centers on certain 35-75 hertz neural oscillations in the cerebral
            cortex; Crick and Koch hypothesize that these oscillations are
            the basis of consciousness. </p>

   <h3 id="BM11">
    18 апреля 1998 </h3>

   <p> Итак, в протокол заносятся не "данные" на каждом
            такте, а "состояния", сохраняющиеся в течение некоторого
            времени. Они всё же похожи на данные. "D растёт" - это
            состояние. D=1 - это данные. "Мишень слева" - это данные
            или состояние? "Мишень слева" - это данные, а "мишень
            обнаружена левым датчиком" - это состояние. А вообще, разница
            между данными, состоянием и процессом такая же, как разница между
            материальным и идеальным. Это субъективно. Следовательно, чем
            модель более "динамична" (хранит динамику данных, а
            не просто данные), тем лучше. </p>

   <p> При "узнавании" текущее состояние сравнивается с
            различными состояниями, хранящимися в протоколе. По команде Push
            текущее состояние вносится в протокол. Если оно отличается от
            последнего зарегистрированного состояния, то создаётся новая "запись"
            с указанием значения, начала, и окончания этого состояния. Если
            это состояние не отличается от последнего, то в последней записи
            изменяется только время окончания. Можно отделить анализ изменения
            от канала. Тогда по сигналу об изменении канал оканчивает предыдущую
            запись и начинает новую. На какое время dt надо делать прогноз? </p>

   <p> Сжатие протокола, или исключение лишних данных происходит
            не по команде Push, а по специальной команде канала Успех. Канал
            успех, вычислив величину успеха за последний контрольный срок,
            определяет, какие данные можно исключить из протокола. Например,
            можно сохранять в самом начале протокола максимально успешные
            данные, а в конце &ndash; все, хотя бы минимально успешные. При этом
            все хранимые данные сравниваются с линейной (монотонной) зависимостью,
            которая максимальна в начале, и минимальна в конце протокола.
            Все данные, успешность которых ниже этой линии, можно исключить. </p>

   <ul> <li> Но при этом может нарушиться важная причинная связь, так как
                после успеха может быть неудача. Поэтому события, непосредственно
                предшествующие успеху, могут быть потеряны. Можно забывать плохой
                успех, если при этом было и плохое самочувствие. При достаточно
                большом удовольствии или успехе опыт сохраняем. Кстати, если каналы
                станут битовыми или аналоговыми, то как хранить удовольствие и
                успех? Как упорядоченные символы? </li> </ul>
   <p> Уровень успеха, который следует сохранять в начале протокола,
            определяется допустимым переполнением протокола. В конце протокола
            (на входе данных) нужно сохранять все данные в течение контрольного
            срока. Сам контрольный срок тоже может изменяться. Если разнообразие
            успехов увеличивается, или максимальный успех слишком часто повторяется,
            то и контрольный срок может увеличиваться, что означает увеличение
            способности к дальнему прогнозу. При увеличении контрольного срока
            меняется максимальный достижимый успех. А если бы успех был "аналоговым"
            каналом, то его можно было анализировать только качественно, а
            не количественно, и проблемы перенормировки успеха не было бы. </p>

   <p> А если успехи начинают уменьшаться, или максимальный успех
            вообще давно уже не повторялся, то контрольный срок тоже должен
            уменьшаться, чтобы увеличить шанс для случайного поиска нового
            более правильного поведения (прогноза). Для начала можно контрольный
            срок не менять. </p>

   <p> По команде "Прогноз" канал находит наиболее вероятное
            событие (состояние), которое следует за предъявленным "коллективно
            узнанным" состоянием. Кроме того, канал находит, насколько
            его узнавание совпало с общим, и вносит величину этой корреляции
            в текущую запись. Надеюсь, это не нужно. Но какова обоснованная
            доля случайности в прогнозе? </p>

   <ul> <li> Менее важный канал, или любой канал при дефиците времени,
                может выполнять "ускоренное узнавание". Это делается
                так. К каждому событию из списка узнаваемых событий прибавляется
                продолжительность одного такта (время от текущего момента до момента
                последнего узнавания). При этом совпадение состояний не проверяется,
                и новые состояния не находятся. Это правило "автоматизма"
                годится и для всего мозга. </li> </ul>
   <p> Такая технология допускает, что обращение к каналам может
            происходить не на каждом такте. Например, если канал, благодаря
            очень длинной истории, уверен, что его предсказание точно, то
            он может не участвовать в прогнозе. Внесение новых данных и прогнозирование
            тоже может происходить не на каждом такте. Но абсолютное физическое
            время всегда должно строго учитываться. Как бы этого избежать? </p>

   <p> На самом деле "старая" технология узнавания тоже
            узнавала события, проходящие с другим темпом. Это получалось потому,
            что узнавание относится ко всей полке с одинаковым значением входящих
            данных Sense. Если текущие данные изменяются, то узнавание теряется
            на всей полке, но сохраняется в последней точке полки, которая
            переходит на следующую по времени полку. Если последняя точка
            текущей полки случайно не была включена в список узнаваемых событий,
            то узнавание могло полностью потеряться. </p>

   <p> Чтобы этого не было надо специально расширять каждое событие
            до ширины полки, на которую оно попало. Это поможет удерживать
            больше узнаваемых событий, чем при чисто случайном поиске. Квантование
            входящих данных уменьшает разнообразие этих данных, и тоже улучшает
            узнавание. </p>

   <p> Если не всю полку, то хотя бы текущую точку узнавания и конец
            полки надо держать наготове. </p>

   <h3 id="BM12">
    Диффузия знаний. </h3>

   <p> Так как замена небольшой доли данных в протоколе не портит
            узнавания, можно рискнуть, и обменивать иногда данные между каналами.
            При этом появляется возможность для "догадок" на основе
            логики, освоенной в другом предметном мире. Но если каналы обслуживают
            физически близкие процессы, например, это два соседних спектральных
            канала в органе слуха, то такой обмен опытом позволяет ускорить
            обучение. </p>

   <h3 id="BM13">
    23 августа 1998. Что надо изменить на веб-странице. </h3>

   <p> Выделить описание бабочки на отдельную страницу. Описать алгоритм
            работы мозга в общих чертах. Написать текст о датчиках. Пояснить
            смысл таких терминов как самопрограммирование, голографическая
            память, переселение сознания. Привести короткий комментарий по
            типичным вопросам о сознании:
    <i> китайская комната </i> , возникновение
            осознавания себя, почему мы осознаём ощущения, как возникает чувство
            жёлтого,
    <i> explanatory gap, the hard problem </i> . Можно поместить
            небольшой перевод из Chalmers'а с комментарием. </p>

   <p> Повторю структуру и распределение функций в свете битовых
            каналов. </p>

   <h4> Структура адаптирующейся системы. </h4>

   <p> Цель системы - как можно более точный и
    <i> выгодный </i> прогноз следующего хода. </p>

   <p> <b> 1. Датчики </b> . Сенсоры и эффекторы, или более общие
            двунаправленные устройства. </p>

   <ul> <li> Датчики связаны друг с другом, с физическими подсистемами,
                и с внешним миром физическими или информационными каналами связи.
                C каналами памяти (протоколами) датчики связаны таким типом данных,
                который может зависеть от физической природы датчика. Особое важное
                место занимают датчики внутреннего состояния. Их показания имеют
                смысл
     <i> величины: </i> от
     <i> хорошо </i> до
     <i> плохо </i> . </li>
    <li> Датчики исполняют команды каналов. </li>
    <li> Один датчик может быть подключен к нескольким каналам для
                передачи разных аспектов своей работы. </li>
    <li> Дублирование каналов и датчиков не запрещено. </li>
    <li> Инерционный датчик может временно терять связь с каналом. </li> </ul>
   <p> <b> 2. Каналы </b> . </p>

   <ul> <li> Не имеют доступа друг к другу, если обслуживают физически
                несводимые датчики. Но могут и использовать опыт соседних каналов,
                если они физически равноправны </li>
    <li> Каждый канал - это синхронная динамическая память (протокол
                событий), которая может содержать несколько битовых каналов. </li>
    <li> Синхронизация данных в каналах обеспечивается либо метками
                времени, которые записываются вместе с данными, либо сохранением
                синхронности номеров записей. </li>
    <li> По команде мозга, канал выполняет прогноз следующего шага
                на основе списка узнанных (похожих) событий текущего
     <i> образа </i> . </li>
    <li> Простейший тип канала - битовый.
     <i> Аналоговый </i> и
     <i> символьный </i> каналы состоят из нескольких битовых. Спектральный
                канал содержит несколько аналоговых каналов. </li> </ul>
   <p> <b> 3. Удовлетворение. </b> Это функция, рассчитываемая на
            основе данных, попавших в некоторые каналы. Обычно большинство
            из каналов, влияющих на удовлетворение, воспринимают данные от
            датчиков внутреннего состояния системы, которое
    <i> имеет смысл </i> . Целью системы является поддержание максимального удовлетворения. </p>

   <ul> <li> Признаков удовлетворения может быть много. В этом случае могут
                возникнуть относительно независимые органы в дополнение к централизованному
                мозгу. </li> </ul>
   <p> <b> 4. Успех </b> . Прогнозирующий механизм стремится обеспечить
            лучшее будущее, характеризуемого тем, что в памяти накапливаются
            события с хорошим качеством будущего. Для этого между возможными
            близкими прогнозами выбираются те, которые обеспечивают лучшее
            будущее, то есть интегрально лучшее удовольствие. Мозг оценивает
            вклад каналов в успех, и присваивает каналам веса важности. </p>

   <blockquote>
    <p> Для оценки "внутреннего состояния системы после события
                t0" анализатор успеха использует значения удовлетворения,
                которые запоминаются в специальном символьном канале с упорядоченными
                символами (числами). Итого мозг содержит два специальных числовых
                канала: Удовольствие и успех. Они могут иметь тип Float. </p>
   </blockquote>

   <h4> Протокол (последовательность) обмена данными </h4>

   <p> 1. Действия каналов </p>

   <ul> <li> Каналы вводят текущую запись в протокол </li> </ul>
   <p> 2. Действия мозга (органа) </p>

   <ul> <li> Уточняет веса каналов по величине успеха, с учётом того,
                какой канал (и с какой точностью) узнавал успех. </li>
    <li> Передаёт каналам списки новых событий и отбирает наиболее
                похожие общепризнанные события. Из них выбираются те, которые
                обеспечивают лучшее будущее. </li> </ul>
   <p> 3. Действия каналов </p>

   <ul> <li> По команде мозга каналы делают прогноз следующего события
                и помещают его во входную запись. </li> </ul>
   <p> 4. Действия датчиков </p>

   <ul> <li> Эффекторы (а, возможно, и сенсоры) читают выходные записи
                своих каналов. </li>
    <li> Эффекторы передают полученную команду своему исполнительному
                устройству (датчику). </li>
    <li> Сенсоры (а, возможно, и эффекторы) вносят изменения в текущие
                записи своих каналов. </li> </ul>
   <p> 5. GOTO 1 </p>

   <p> <a href="../post/email.html" rel="author" title="email и копирайт">
     Евгений Корниенко </a> </p>
  </main>

  <div id="menu">
  </div>

  <footer>
  </footer>

  <noscript>
   <div>
    
   </div> </noscript>
 </body>
</html>